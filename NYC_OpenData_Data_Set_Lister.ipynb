{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "NYC OpenData Data Set Lister.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.8 64-bit (conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NYC OpenData: Data Set Lister\r\n",
        "This script lists all the data sets given by a NYC OpenData URL. (The script scrapes the website successfully as of September 20, 2021.)"
      ],
      "metadata": {
        "id": "YckoFp3Zvm7E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install esprima\r\n",
        "import re\r\n",
        "import requests\r\n",
        "import esprima\r\n",
        "import json\r\n",
        "import time\r\n",
        "from datetime import datetime, timezone\r\n",
        "from os.path import isfile\r\n",
        "from urllib.parse import urljoin\r\n",
        "from bs4 import BeautifulSoup, Tag\r\n",
        "from itertools import chain, dropwhile\r\n",
        "from random import randint\r\n",
        "from pprint import pprint\r\n",
        "\r\n",
        "#@title Settings\r\n",
        "url = \"https://data.cityofnewyork.us/browse?Dataset-Information_Agency=Department+of+Education+%28DOE%29&sortBy=alpha&utf8=%E2%9C%93\" #@param {type:\"string\"}\r\n",
        "#@markdown When providing the `url`, note that by default, the search results \r\n",
        "#@markdown are ordered by “relevance” which will likely cause the order of the \r\n",
        "#@markdown results to change while parsing multiple search result pages. \r\n",
        "#@markdown Consequently, it’s possible to parse all of the pages in the set and \r\n",
        "#@markdown still not have *all* of the search results. Use a sort order like \r\n",
        "#@markdown “alphabetical” for more stable results.\r\n",
        "cache = \"cache.json\" #@param {type:\"string\"}\r\n",
        "use_cache = \"Yes\" #@param [\"Yes\", \"No\"]\r\n",
        "#@markdown The `use_cache` setting above only affects getting search results.\r\n",
        "#@markdown Activating the setting means the provided `url` will *not* be used if\r\n",
        "#@markdown the cache file exists.\r\n",
        "\r\n",
        "# A regular expression to match all whitespace except line breaks\r\n",
        "whitespace_re = re.compile(r'[^\\S\\r\\n]+', re.DOTALL)\r\n",
        "\r\n",
        "# A regular expression to match “# results”\r\n",
        "results_count_re = re.compile(r'(\\d+)\\s+Results|(1)\\s+Result', re.IGNORECASE)\r\n",
        "\r\n",
        "# Loads a JSON a file\r\n",
        "def load_from_cache(path):\r\n",
        "  with open(path, 'r') as cache_file:\r\n",
        "    return json.load(cache_file)\r\n",
        "\r\n",
        "# Gets the first item in a subscriptable object or None\r\n",
        "def first(iterable):\r\n",
        "  return next(iter(iterable), None)\r\n",
        "\r\n",
        "# Gets the first non-null item in a subscriptable object or None\r\n",
        "def first_nonnull(iterable):\r\n",
        "  return first(dropwhile(lambda item: item is None, iterable))\r\n",
        "\r\n",
        "# Removes excess whitespace inside and out\r\n",
        "def strip(value):\r\n",
        "  return whitespace_re.sub(' ', value.strip())\r\n",
        "\r\n",
        "# Converts UNIX time to an ISO format string\r\n",
        "def unix_time_to_iso(timestamp):\r\n",
        "  return datetime.fromtimestamp(timestamp, timezone.utc).isoformat()\r\n",
        "\r\n",
        "# Scrapes the supplied NYC OpenData *browse* url for items\r\n",
        "def get_data_sets(url, data_sets={}):\r\n",
        "  delay_factor = 1 # This delay factor will increase exponentially on errors.\r\n",
        "  sleep = lambda: time.sleep(randint(2, 4) * delay_factor)\r\n",
        "  print(f'Getting search results from {url}...')\r\n",
        "  try:\r\n",
        "    page_data_sets, next_url, last_url, expected_length = parse_results_page(url)\r\n",
        "    page_data_sets.update(data_sets) # Data sets already in the collection\r\n",
        "    data_sets = page_data_sets       # override the freshly extracted ones.\r\n",
        "    print(f'Expecting a total of {expected_length} search results...')\r\n",
        "    sleep()\r\n",
        "    while next_url:\r\n",
        "      print(f'Getting additional search results from {next_url}...')\r\n",
        "      try:\r\n",
        "        url = next_url\r\n",
        "        page_data_sets, next_url, new_last_url, new_expected_length \\\r\n",
        "          = parse_results_page(next_url)\r\n",
        "        page_data_sets.update(data_sets) # Data sets already in the collection\r\n",
        "        data_sets = page_data_sets       # override the freshly extracted ones.\r\n",
        "        if expected_length != new_expected_length:\r\n",
        "          print(f'The number of search results provided by the server changed '\r\n",
        "                f'from {expected_length} to {new_expected_length}!')\r\n",
        "          expected_length = new_expected_length\r\n",
        "        if next_url:\r\n",
        "          delay_factor = max(1, delay_factor // 2)\r\n",
        "          sleep() # Take it nice and easy; the server will be angry otherwise.\r\n",
        "        elif last_url and last_url != url:\r\n",
        "          print(f'last_url = {last_url}, url = {url}')\r\n",
        "          print('The server returned unexpected results. Could not extract a '\r\n",
        "                'link to the next search results page. The returned results may'\r\n",
        "                ' not be complete.')\r\n",
        "      except Exception as e:\r\n",
        "        print(f'An error occured while getting additional search results: {e}')\r\n",
        "        print('The returned results may not be complete.')\r\n",
        "        delay_factor = min(1800, delay_factor * 2)\r\n",
        "        sleep()\r\n",
        "    print(f'Finished extracting {len(data_sets)} search results.')\r\n",
        "    if len(data_sets) != expected_length:\r\n",
        "      print(f'The number of search results extracted did not match the expected'\r\n",
        "            f' number of results ({expected_length}).')\r\n",
        "    return data_sets\r\n",
        "  except Exception as e:\r\n",
        "    print(f'An error occured while getting the search results: {e}')\r\n",
        "\r\n",
        "# Loads and parses the results page, returning the items and the next page’s url\r\n",
        "def parse_results_page(url):\r\n",
        "  make_full_url = lambda relative_url: urljoin(url, relative_url) \\\r\n",
        "                                       if relative_url \\\r\n",
        "                                       else None\r\n",
        "  response = requests.get(url)\r\n",
        "  response.raise_for_status() # Raises an error if the request is not successful\r\n",
        "  soup = BeautifulSoup(response.text) # Parses the raw HTML into a structure\r\n",
        "  data_sets = {result_element.get('data-view-id'):\r\n",
        "                   element_to_dict(result_element)\r\n",
        "               for result_element\r\n",
        "               in soup.select('.browse2-result')}\r\n",
        "  next_url = make_full_url(extract_next_url(soup))\r\n",
        "  last_url = make_full_url(extract_last_url(soup))\r\n",
        "  expected_length = extract_expected_length(soup)\r\n",
        "  return (data_sets, next_url, last_url, expected_length)\r\n",
        "\r\n",
        "# Extracts information about each result into a dictionary\r\n",
        "def element_to_dict(element):\r\n",
        "  def get_element(selector):\r\n",
        "    return first(element.select(selector))\r\n",
        "  def get_link(selector):\r\n",
        "    element = get_element(selector)\r\n",
        "    if element:\r\n",
        "      return element.get('href')\r\n",
        "  def get_text(selector):\r\n",
        "    element = get_element(selector)\r\n",
        "    if element:\r\n",
        "      return strip(element.text)\r\n",
        "  timestamp = get_element('.browse2-result-timestamp-value > '\r\n",
        "                        + '[data-rawdatetime]') \\\r\n",
        "             .get('data-rawdatetime')\r\n",
        "  return {'name': get_text('.browse2-result-name-link'),\r\n",
        "          'link': get_link('.browse2-result-name-link'),\r\n",
        "          'category': get_text('.browse2-result-category'),\r\n",
        "          'type': get_text('.browse2-result-type-name'),\r\n",
        "          'description': get_text('.browse2-result-description'),\r\n",
        "          'tags': list(map(Tag.get_text, element.select('.browse2-result-topic'))),\r\n",
        "          'updated': unix_time_to_iso(int(timestamp)),\r\n",
        "          'apiDocLink': get_link('.browse2-result-api-link')}\r\n",
        "\r\n",
        "# Adds details to each item by modifying its dictionary in-place\r\n",
        "def get_details(data_sets):\r\n",
        "  delay_factor = 1 # This delay factor will increase exponentially on errors.\r\n",
        "  sleep = lambda: time.sleep(randint(2, 4) * delay_factor)\r\n",
        "  for id in data_sets:\r\n",
        "    data_set = data_sets[id]\r\n",
        "    if 'dataDownloads' in data_set or \\\r\n",
        "       'attachments' in data_set or \\\r\n",
        "       'columns' in data_set:\r\n",
        "       continue # Skip items with any of those keys already.\r\n",
        "    try:\r\n",
        "      data_set_name = data_set['name']\r\n",
        "      details_url = data_set['link']\r\n",
        "      print(f'Getting details for {data_set_name} from {details_url}...')\r\n",
        "      sleep() # Take it nice and easy; the server will be angry otherwise.\r\n",
        "      data_set_information, initial_state = parse_details_page(details_url)\r\n",
        "      data_sets[id]['rawMetadata'] = {'dataSet': data_set_information,\r\n",
        "                                      'initialState': initial_state}\r\n",
        "      data_downloads = extract_data_downloads(url, data_set_information)\r\n",
        "      attachments = extract_attachments(url, initial_state)\r\n",
        "      columns = extract_column_schema(initial_state)\r\n",
        "      data_sets[id]['attachments'] = attachments\r\n",
        "      data_sets[id]['columns'] = columns\r\n",
        "      data_sets[id]['dataDownloads'] = data_downloads\r\n",
        "      delay_factor = max(1, delay_factor // 2)\r\n",
        "    except ValueError as e:\r\n",
        "      print(f'\\tAn error occured while getting the details: {e}')\r\n",
        "      delay_factor = min(300, delay_factor * 2)\r\n",
        "\r\n",
        "# Extracts the data downloads information associated with the item\r\n",
        "def extract_data_downloads(url_base, data_set_information):\r\n",
        "  if data_set_information and data_set_information['distribution']:\r\n",
        "    return [{'contentUrl': data_download['contentUrl'],\r\n",
        "             'encodingFormat': data_download['encodingFormat']}\r\n",
        "            for data_download\r\n",
        "            in data_set_information['distribution']\r\n",
        "            if data_download['@type'] == 'DataDownload']\r\n",
        "\r\n",
        "# Extracts the attachments information associated with the item\r\n",
        "def extract_attachments(url_base, initial_state):\r\n",
        "  if initial_state and \\\r\n",
        "     initial_state['view'] and \\\r\n",
        "     initial_state['view']['attachments']:\r\n",
        "    return {attachment['name']: urljoin(url_base, attachment['href'])\r\n",
        "            for attachment\r\n",
        "            in initial_state['view']['attachments']}\r\n",
        "\r\n",
        "# Extracts the column schema information associated with the item\r\n",
        "def extract_column_schema(initial_state):\r\n",
        "  if initial_state and \\\r\n",
        "     initial_state['view'] and \\\r\n",
        "     initial_state['view']['columns']:\r\n",
        "    return [{'name': column['fieldName'],\r\n",
        "             'type': column['dataTypeName'],\r\n",
        "             'humanName': column['name']}\r\n",
        "            for column\r\n",
        "            in sorted(initial_state['view']['columns'],\r\n",
        "                      key=lambda _: int(_['position']))]\r\n",
        "\r\n",
        "# Digs into the item’s page to extract additional details\r\n",
        "def parse_details_page(url):\r\n",
        "  response = requests.get(url)\r\n",
        "  response.raise_for_status()\r\n",
        "  soup = BeautifulSoup(response.text)\r\n",
        "  data_set_information = None # The information about the data set embedded in the page\r\n",
        "  json_objects = extract_inline_json(soup)\r\n",
        "  for json_object in json_objects:\r\n",
        "    if json_object and json_object['@type'] == 'Dataset': # Found the information!\r\n",
        "      data_set_information = json_object\r\n",
        "      break\r\n",
        "  initial_state = None # The embedded information that the page uses to initialize its tables\r\n",
        "  scripts = extract_inline_javascript(soup)\r\n",
        "  for script in scripts:\r\n",
        "    try:\r\n",
        "      ast = esprima.parseScript(script, {'range': True}) # Guards against not-JavaScript\r\n",
        "      if ast.type == 'Program' and \\\r\n",
        "         ast.sourceType == 'script' and \\\r\n",
        "         len(ast.body) == 1 and \\\r\n",
        "         ast.body[0].type == 'VariableDeclaration' and \\\r\n",
        "         len(ast.body[0].declarations) == 1: # Found the script with a single variable declaration\r\n",
        "        declaration = ast.body[0].declarations[0]\r\n",
        "        if declaration.type == 'VariableDeclarator' and \\\r\n",
        "           declaration.id.type == 'Identifier' and \\\r\n",
        "           declaration.id.name == 'initialState' and \\\r\n",
        "           declaration.init.type == 'ObjectExpression': # Found the initial state!\r\n",
        "          json_start, json_end = declaration.init.range\r\n",
        "          initial_state = json.loads(script[json_start:json_end])\r\n",
        "          break\r\n",
        "    except Exception as e:\r\n",
        "      pass # Ignore the “script” if parsing it throws an exception.\r\n",
        "  return (data_set_information, initial_state)\r\n",
        "\r\n",
        "# Loads all of the inline JSON found in the page’s script tags\r\n",
        "def extract_inline_json(soup):\r\n",
        "  return [json.loads(element.string)\r\n",
        "          for element\r\n",
        "          in soup.select('script[type=\"application/ld+json\"]')\r\n",
        "          if element.string]\r\n",
        "\r\n",
        "# Loads all of the inline JavaScript found in the page’s script tags\r\n",
        "def extract_inline_javascript(soup):\r\n",
        "  return [element.string\r\n",
        "          for element\r\n",
        "          in soup.select('script')\r\n",
        "          if element.string]\r\n",
        "\r\n",
        "# Gets the URL of the next results page\r\n",
        "def extract_next_url(soup):\r\n",
        "  element = soup.select('a.nextLink')\r\n",
        "  if element:\r\n",
        "    return element[0].get('href')\r\n",
        "\r\n",
        "# Gets the URL of the last results page\r\n",
        "def extract_last_url(soup):\r\n",
        "  element = soup.select('a.lastLink')\r\n",
        "  if element:\r\n",
        "    return element[0].get('href')\r\n",
        "\r\n",
        "# Gets the number of search results to expect\r\n",
        "def extract_expected_length(soup):\r\n",
        "  element = first(soup.select('.browse2-mobile-filter-result-count'))\r\n",
        "  return int(first_nonnull(first(map(re.Match.groups,\r\n",
        "                                     filter(None,\r\n",
        "                                            map(results_count_re.match,\r\n",
        "                                                element.stripped_strings))))))"
      ],
      "outputs": [],
      "metadata": {
        "id": "ub8rvxXfvRsN",
        "cellView": "form"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting the Search Results\n",
        "The code below extracts the data from the first search results page and all subsequent pages until it cannot find a link to the next page. For each page, it looks for the search result elements and maps each one to a dictionary. The dictionary schema is as follows:\n",
        "* **name** (string): the name of the item\n",
        "* **link** (string): the link to the page with more information about the item\n",
        "* **category** (string): the category of the item (e.g., *Education*)\n",
        "* **type** (string): the type of the item (e.g., *Dataset*)\n",
        "* **description** (string): a description of the item\n",
        "* **tags** (set of strings): a set of tags associated with the item\n",
        "* **updated** (integer): the timestamp which this item was last updated\n",
        "* **apiDocLink** (string): a link to the API documentation (which might possibly be used to extract more metadata about the item)"
      ],
      "metadata": {
        "id": "4FiTW1vjXoPZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "data_sets = load_from_cache(cache) if use_cache == 'Yes' and isfile(cache) else get_data_sets(url)"
      ],
      "outputs": [],
      "metadata": {
        "id": "v8PPw6lnwOO-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22cb392b-f1fe-46ab-d093-0932d816224a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting the Data Set Details\n",
        "The code below extracts additional information about the items using the links to the items’ pages. It adds the following keys if the information is available:\n",
        "* **attachments** (dict of strings): key-value pairs of file names and their corresponding links to download them\n",
        "* **columns** (list of dicts): an ordered list of dicts representing column metadata\n",
        "* **dataDownloads** (list of dicts of strings): a list of key-value pairs where the key is the file name and the value is the link to its URL\n",
        "\n",
        "In the case where download of data set details is interrupted, the code below will attempt to resume progress. Simply, it checks each dictionary entry for the existence of the additional keys. If those don’t exist, it tries to retrieve them again and amends the dictionary."
      ],
      "metadata": {
        "id": "U8nQnEB4K0Pr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "get_details(data_sets)"
      ],
      "outputs": [],
      "metadata": {
        "id": "2-2QbHRsMS4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Caching Data\n",
        "The code below will save a copy of the data to storage for loading and processing later. The file name is defined in the settings above."
      ],
      "metadata": {
        "id": "bnTGn10FSikm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "with open(cache, 'w') as cache_file:\r\n",
        "  json.dump(data_sets, cache_file, indent=2, sort_keys=True)"
      ],
      "outputs": [],
      "metadata": {
        "id": "UZz_9mslTlNi"
      }
    }
  ]
}