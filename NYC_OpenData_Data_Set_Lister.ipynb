{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NYC OpenData Data Set Lister.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YckoFp3Zvm7E"
      },
      "source": [
        "# NYC OpenData: Data Set Lister\n",
        "This script lists all the data sets given by a NYC OpenData URL. (The script scrapes the website successfully as of September 14, 2021.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ub8rvxXfvRsN",
        "cellView": "form"
      },
      "source": [
        "!pip install esprima\n",
        "import re\n",
        "import requests\n",
        "import esprima\n",
        "import json\n",
        "from os.path import isfile\n",
        "from urllib.parse import urljoin\n",
        "from time import sleep\n",
        "from bs4 import BeautifulSoup, Tag\n",
        "from itertools import chain, islice\n",
        "from random import randint\n",
        "from pprint import pprint\n",
        "\n",
        "#@title Settings\n",
        "url = \"https://data.cityofnewyork.us/browse?tags=permit\" #@param {type:\"string\"}\n",
        "cache = \"cache.json\" #@param {type:\"string\"}\n",
        "use_cache = \"Yes\" #@param [\"Yes\", \"No\"]\n",
        "#@markdown The `use_cache` setting above only affects getting search results.\n",
        "#@markdown Activating the setting means the provided `url` will *not* be used if\n",
        "#@markdown the cache file exists.\n",
        "\n",
        "# A regular expression to match all whitespace except line breaks\n",
        "whitespace_re = re.compile(r'[^\\S\\r\\n]+', re.DOTALL)\n",
        "\n",
        "# Loads a JSON a file\n",
        "def load_from_cache(path):\n",
        "  with open(path, 'r') as cache_file:\n",
        "    return json.load(cache_file)\n",
        "\n",
        "# Gets the first item in a subscriptable object or None\n",
        "def first(x):\n",
        "  if x:\n",
        "    return x[0]\n",
        "\n",
        "# Removes excess whitespace inside and out\n",
        "def strip(value):\n",
        "  return whitespace_re.sub(' ', value.strip())\n",
        "\n",
        "# Scrapes the supplied NYC OpenData *browse* url for items\n",
        "def get_data_sets(url):\n",
        "  delay_factor = 1 # This delay factor will increase exponentially on errors.\n",
        "  data_sets = {}\n",
        "  next_url = url\n",
        "  while next_url:\n",
        "    print(f'Getting search results from {next_url}...')\n",
        "    try:\n",
        "      page_data_sets, next_url = parse_results_page(next_url)\n",
        "      data_sets.update(page_data_sets)\n",
        "      if next_url:\n",
        "        next_url = urljoin(url, next_url)\n",
        "        delay_factor = max(1, delay_factor // 2)\n",
        "        sleep(randint(2, 4) * delay_factor) # Take it nice and easy;\n",
        "                                            # the server will be angry otherwise.\n",
        "    except Exception as e:\n",
        "      print(f'An error occured while getting the search results: {e}')\n",
        "      delay_factor = min(1800, delay_factor * 2)\n",
        "      sleep(randint(3, 7) * delay_factor)\n",
        "  print(f'Finished extracting {len(data_sets)} search results.')\n",
        "  return data_sets\n",
        "\n",
        "# Loads and parses the results page, returning the items and the next page’s url\n",
        "def parse_results_page(url):\n",
        "  response = requests.get(url)\n",
        "  response.raise_for_status() # Raises an error if the request is not successful\n",
        "  soup = BeautifulSoup(response.text) # Parses the raw HTML into a structure\n",
        "  data_sets = {result_element.get('data-view-id'): element_to_dict(result_element)\n",
        "               for result_element\n",
        "               in soup.select('.browse2-result')}\n",
        "  next_url = extract_next_url(soup)\n",
        "  return (data_sets, next_url)\n",
        "\n",
        "# Extracts information about each result into a dictionary\n",
        "def element_to_dict(element):\n",
        "  def get_element(selector):\n",
        "    return first(element.select(selector))\n",
        "  def get_link(selector):\n",
        "    element = get_element(selector)\n",
        "    if element:\n",
        "      return element.get('href')\n",
        "  def get_text(selector):\n",
        "    element = get_element(selector)\n",
        "    if element:\n",
        "      return strip(element.text)\n",
        "  timestamp_element = get_element('.browse2-result-timestamp-value > '\n",
        "                                + '[data-rawdatetime]')\n",
        "  return {'name': get_text('.browse2-result-name-link'),\n",
        "          'link': get_link('.browse2-result-name-link'),\n",
        "          'category': get_text('.browse2-result-category'),\n",
        "          'type': get_text('.browse2-result-type-name'),\n",
        "          'description': get_text('.browse2-result-description'),\n",
        "          'tags': list(map(Tag.get_text, element.select('.browse2-result-topic'))),\n",
        "          'updated': int(timestamp_element.get('data-rawdatetime')), # UNIX time\n",
        "          'apiDocLink': get_link('.browse2-result-api-link')}\n",
        "\n",
        "# Adds details to each item by modifying its dictionary in-place\n",
        "def get_details(data_sets):\n",
        "  delay_factor = 1 # This delay factor will increase exponentially on errors.\n",
        "  for id in data_sets:\n",
        "    data_set = data_sets[id]\n",
        "    if 'dataDownloads' in data_set or \\\n",
        "       'attachments' in data_set or \\\n",
        "       'columns' in data_set:\n",
        "       continue # Skip items with any of those keys already.\n",
        "    try:\n",
        "      data_set_name = data_set['name']\n",
        "      details_url = data_set['link']\n",
        "      print(f'Getting details for {data_set_name} from {details_url}...')\n",
        "      sleep(randint(2, 4) * delay_factor) # Take it nice and easy;\n",
        "                                          # the server will be angry otherwise.\n",
        "      data_set_information, initial_state = parse_details_page(details_url)\n",
        "      if data_set_information and data_set_information['distribution']:\n",
        "        data_downloads = [{'contentUrl': data_download['contentUrl'],\n",
        "                           'encodingFormat': data_download['encodingFormat']}\n",
        "                          for data_download\n",
        "                          in data_set_information['distribution']]\n",
        "        if data_downloads:\n",
        "          print(f'\\t{data_set_name} has {len(data_downloads)} data downloads.')\n",
        "          data_sets[id]['dataDownloads'] = data_downloads\n",
        "        else:\n",
        "          print(f'\\tNo data downloads were found for {data_set_name}.')\n",
        "      attachments = extract_attachments(url, initial_state)\n",
        "      if attachments:\n",
        "        print(f'\\t{data_set_name} has {len(attachments)} attachments.')\n",
        "        data_sets[id]['attachments'] = attachments\n",
        "      else:\n",
        "        print(f'\\tNo attachments were found for {data_set_name}.')\n",
        "      columns = extract_column_schema(initial_state)\n",
        "      if columns:\n",
        "        print(f'\\t{data_set_name} has {len(columns)} columns.')\n",
        "        data_sets[id]['columns'] = columns\n",
        "      else:\n",
        "        print(f'\\tNo columns were found for {data_set_name}.')\n",
        "      delay_factor = max(1, delay_factor // 2)\n",
        "    except Exception as e:\n",
        "      print(f'An error occured while getting the details: {e}')\n",
        "      delay_factor = min(300, delay_factor * 2)\n",
        "\n",
        "# Extracts the attachments information associated with the item\n",
        "def extract_attachments(url_base, initial_state):\n",
        "  if initial_state['view'] and initial_state['view']['attachments']:\n",
        "    return {attachment['name']: urljoin(url_base, attachment['href'])\n",
        "            for attachment\n",
        "            in initial_state['view']['attachments']}\n",
        "\n",
        "# Extracts the column schema information associated with the item\n",
        "def extract_column_schema(initial_state):\n",
        "  if initial_state['view'] and initial_state['view']['columns']:\n",
        "    return [{'name': column['fieldName'],\n",
        "             'type': column['dataTypeName'],\n",
        "             'humanName': column['name']}\n",
        "            for column\n",
        "            in sorted(initial_state['view']['columns'],\n",
        "                      key=lambda _: int(_['position']))]\n",
        "\n",
        "# Digs into the item’s page to extract additional details\n",
        "def parse_details_page(url):\n",
        "  response = requests.get(url)\n",
        "  response.raise_for_status()\n",
        "  soup = BeautifulSoup(response.text)\n",
        "  data_set_information = None # The information about the data set embedded in the page\n",
        "  json_objects = extract_inline_json(soup)\n",
        "  for json_object in json_objects:\n",
        "    if json_object and json_object['@type'] == 'Dataset': # Found the information!\n",
        "      data_set_information = json_object # Grab that JSON.\n",
        "      break\n",
        "  initial_state = None # The embedded information that the page uses to initialize its tables\n",
        "  scripts = extract_inline_javascript(soup)\n",
        "  for script in scripts:\n",
        "    try:\n",
        "      ast = esprima.parseScript(script, {'range': True}) # Guards against not-JavaScript\n",
        "      if ast.type == 'Program' and \\\n",
        "         ast.sourceType == 'script' and \\\n",
        "         len(ast.body) == 1 and \\\n",
        "         ast.body[0].type == 'VariableDeclaration' and \\\n",
        "         len(ast.body[0].declarations) == 1: # Found the script with a single variable declaration\n",
        "        declaration = ast.body[0].declarations[0]\n",
        "        if declaration.type == 'VariableDeclarator' and \\\n",
        "           declaration.id.type == 'Identifier' and \\\n",
        "           declaration.id.name == 'initialState' and \\\n",
        "           declaration.init.type == 'ObjectExpression': # Found the initial state!\n",
        "          json_start, json_end = declaration.init.range # Get the JSON range.\n",
        "          initial_state = json.loads(script[json_start:json_end]) # Load the JSON.\n",
        "          break\n",
        "    except Exception as e:\n",
        "      pass # Ignore the “script” if parsing it throws an exception.\n",
        "  return (data_set_information, initial_state)\n",
        "\n",
        "# Loads all of the inline JSON found in the page’s script tags\n",
        "def extract_inline_json(soup):\n",
        "  return [json.loads(element.text)\n",
        "          for element\n",
        "          in soup.select('script[type=\"application/ld+json\"]')\n",
        "          if element.text]\n",
        "\n",
        "# Loads all of the inline JavaScript found in the page’s script tags\n",
        "def extract_inline_javascript(soup):\n",
        "  return [element.text for element in soup.select('script') if element.text]\n",
        "\n",
        "# Gets the URL of the next results page\n",
        "def extract_next_url(soup):\n",
        "  element = soup.select('a.nextLink')\n",
        "  if element:\n",
        "    return element[0].get('href')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FiTW1vjXoPZ"
      },
      "source": [
        "## Getting the Search Results\n",
        "The code below extracts the data from the first search results page and all subsequent pages until it cannot find a link to the next page. For each page, it looks for the search result elements and maps each one to a dictionary. The dictionary schema is as follows:\n",
        "* **name** (string): the name of the item\n",
        "* **link** (string): the link to the page with more information about the item\n",
        "* **category** (string): the category of the item (e.g., *Education*)\n",
        "* **type** (string): the type of the item (e.g., *Dataset*)\n",
        "* **description** (string): a description of the item\n",
        "* **tags** (set of strings): a set of tags associated with the item\n",
        "* **updated** (integer): the UNIX timestamp which this item was last updated\n",
        "* **apiDocLink** (string): a link to the API documentation (which might possibly be used to extract more metadata about the item)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8PPw6lnwOO-"
      },
      "source": [
        "data_sets = load_from_cache(cache) if use_cache == 'Yes' and isfile(cache) else get_data_sets(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8nQnEB4K0Pr"
      },
      "source": [
        "## Getting the Data Set Details\n",
        "The code below extracts additional information about the items using the links to the items’ pages. It adds the following keys if the information is available:\n",
        "* **attachments** (dict of strings): key-value pairs of file names and their corresponding links to download them\n",
        "* **columns** (list of dicts): an ordered list of dicts representing column metadata\n",
        "* **dataDownloads** (list of dicts of strings): a list of key-value pairs where the key is the file name and the value is the link to its URL\n",
        "\n",
        "In the case where download of data set details is interrupted, the code below will attempt to resume progress. Simply, it checks each dictionary entry for the existence of the additional keys. If those don’t exist, it tries to retrieve them again and amends the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-2QbHRsMS4W"
      },
      "source": [
        "get_details(data_sets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY4VPpeYH7dz"
      },
      "source": [
        "## What’s Inside the Data Set?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBYJCnY3IBn8"
      },
      "source": [
        "pprint(data_sets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnTGn10FSikm"
      },
      "source": [
        "## Caching Data\n",
        "The code below will save a copy of the data to storage for loading and processing later. The file name is defined in the settings above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZz_9mslTlNi"
      },
      "source": [
        "with open(cache, 'w') as cache_file:\n",
        "  json.dump(data_sets, cache_file)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}