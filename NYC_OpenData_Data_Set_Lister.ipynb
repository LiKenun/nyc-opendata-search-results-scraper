{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "NYC OpenData Data Set Lister.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.8 64-bit ('base': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NYC OpenData: Data Set Lister\r\n",
        "This script lists all the data sets given by a NYC OpenData URL. (The script scrapes the website successfully as of September 27, 2021.)"
      ],
      "metadata": {
        "id": "YckoFp3Zvm7E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing and Loading Libraries"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install beautifulsoup4\r\n",
        "!pip install esprima\r\n",
        "import json\r\n",
        "import re\r\n",
        "import requests\r\n",
        "import time\r\n",
        "import esprima\r\n",
        "from datetime import datetime, timezone\r\n",
        "from functools import partial\r\n",
        "from itertools import dropwhile\r\n",
        "from os.path import isfile\r\n",
        "from random import randint\r\n",
        "from urllib.parse import urljoin\r\n",
        "from bs4 import BeautifulSoup, Tag"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Settings and Functions"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Settings\r\n",
        "url = \"https://data.cityofnewyork.us/browse?Dataset-Information_Agency=Department+of+Education+%28DOE%29&sortBy=alpha&utf8=%E2%9C%93\" #@param {type:\"string\"}\r\n",
        "#@markdown When providing the `url`, note that by default, the search results \r\n",
        "#@markdown are ordered by “relevance” which will likely cause the order of the \r\n",
        "#@markdown results to change while parsing multiple search result pages. \r\n",
        "#@markdown Consequently, it’s possible to parse all of the pages in the set and \r\n",
        "#@markdown still not have *all* of the search results. Use a sort order like \r\n",
        "#@markdown “alphabetical” for more stable results.\r\n",
        "cache = \"cache.json\" #@param {type:\"string\"}\r\n",
        "use_cache = \"Yes\" #@param [\"Yes\", \"No\"]\r\n",
        "#@markdown The `use_cache` setting above only affects getting search results.\r\n",
        "#@markdown Activating the setting means the provided `url` will *not* be used if\r\n",
        "#@markdown the cache file exists.\r\n",
        "\r\n",
        "# A regular expression to match all whitespace except line breaks\r\n",
        "whitespace_re = re.compile(r'[^\\S\\r\\n]+', re.DOTALL)\r\n",
        "\r\n",
        "# A regular expression to match “# results”\r\n",
        "results_count_re = re.compile(r'(\\d+)\\s+Results|(1)\\s+Result', re.IGNORECASE)\r\n",
        "\r\n",
        "# Loads a JSON a file\r\n",
        "def load_from_cache(path):\r\n",
        "  with open(path, 'r') as cache_file:\r\n",
        "    return json.load(cache_file)\r\n",
        "\r\n",
        "# Gets the first item in a subscriptable object or None\r\n",
        "def first(iterable):\r\n",
        "  return next(iter(iterable), None)\r\n",
        "\r\n",
        "# Gets the first non-null item in a subscriptable object or None\r\n",
        "def first_nonnull(iterable):\r\n",
        "  return first(dropwhile(lambda item: item is None, iterable))\r\n",
        "\r\n",
        "# Removes excess whitespace inside and out\r\n",
        "def strip(value):\r\n",
        "  return whitespace_re.sub(' ', value.strip())\r\n",
        "\r\n",
        "# Converts UNIX time to an ISO format string\r\n",
        "def unix_time_to_iso(timestamp):\r\n",
        "  return datetime.fromtimestamp(timestamp, timezone.utc).isoformat()\r\n",
        "\r\n",
        "# Scrapes the supplied NYC OpenData *browse* url for items\r\n",
        "def get_data_sets(url, data_sets={}):\r\n",
        "  delay_factor = 1 # This delay factor will increase exponentially on errors.\r\n",
        "  sleep = lambda: time.sleep(randint(2, 4) * delay_factor)\r\n",
        "  print(f'Getting search results from {url}...')\r\n",
        "  try:\r\n",
        "    page_data_sets, next_url, last_url, expected_length = parse_results_page(url)\r\n",
        "    page_data_sets.update(data_sets) # Data sets already in the collection\r\n",
        "    data_sets = page_data_sets       # override the freshly extracted ones.\r\n",
        "    print(f'Expecting a total of {expected_length} search results...')\r\n",
        "    sleep()\r\n",
        "    while next_url:\r\n",
        "      print(f'Getting additional search results from {next_url}...')\r\n",
        "      try:\r\n",
        "        url = next_url\r\n",
        "        page_data_sets, next_url, new_last_url, new_expected_length \\\r\n",
        "          = parse_results_page(next_url)\r\n",
        "        page_data_sets.update(data_sets) # Data sets already in the collection\r\n",
        "        data_sets = page_data_sets       # override the freshly extracted ones.\r\n",
        "        if expected_length != new_expected_length:\r\n",
        "          print(f'The number of search results provided by the server changed '\r\n",
        "                f'from {expected_length} to {new_expected_length}!')\r\n",
        "          expected_length = new_expected_length\r\n",
        "        if next_url:\r\n",
        "          delay_factor = max(1, delay_factor // 2)\r\n",
        "          sleep() # Take it nice and easy; the server will be angry otherwise.\r\n",
        "        elif last_url and last_url != url:\r\n",
        "          print(f'last_url = {last_url}, url = {url}')\r\n",
        "          print('The server returned unexpected results. Could not extract a '\r\n",
        "                'link to the next search results page. The returned results may'\r\n",
        "                ' not be complete.')\r\n",
        "      except Exception as e:\r\n",
        "        print(f'An error occured while getting additional search results: {e}')\r\n",
        "        print('The returned results may not be complete.')\r\n",
        "        delay_factor = min(1800, delay_factor * 2)\r\n",
        "        sleep()\r\n",
        "    print(f'Finished extracting {len(data_sets)} search results.')\r\n",
        "    if len(data_sets) != expected_length:\r\n",
        "      print(f'The number of search results extracted did not match the expected'\r\n",
        "            f' number of results ({expected_length}).')\r\n",
        "    return data_sets\r\n",
        "  except Exception as e:\r\n",
        "    print(f'An error occured while getting the search results: {e}')\r\n",
        "\r\n",
        "# Loads and parses the results page, returning the items and the next page’s url\r\n",
        "def parse_results_page(url):\r\n",
        "  make_full_url = lambda relative_url: urljoin(url, relative_url) \\\r\n",
        "                                       if relative_url \\\r\n",
        "                                       else None\r\n",
        "  response = requests.get(url)\r\n",
        "  response.raise_for_status() # Raises an error if the request is not successful\r\n",
        "  soup = BeautifulSoup(response.text) # Parses the raw HTML into a structure\r\n",
        "  data_sets = {result_element.get('data-view-id'):\r\n",
        "                   element_to_dict(result_element)\r\n",
        "               for result_element\r\n",
        "               in soup.select('.browse2-result')}\r\n",
        "  next_url = make_full_url(extract_next_url(soup))\r\n",
        "  last_url = make_full_url(extract_last_url(soup))\r\n",
        "  expected_length = extract_expected_length(soup)\r\n",
        "  return (data_sets, next_url, last_url, expected_length)\r\n",
        "\r\n",
        "# Extracts information about each result into a dictionary\r\n",
        "def element_to_dict(element):\r\n",
        "  def get_element(selector):\r\n",
        "    return first(element.select(selector))\r\n",
        "  def get_link(selector):\r\n",
        "    element = get_element(selector)\r\n",
        "    if element:\r\n",
        "      return element.get('href')\r\n",
        "  def get_text(selector):\r\n",
        "    element = get_element(selector)\r\n",
        "    if element:\r\n",
        "      return strip(element.text)\r\n",
        "  timestamp = get_element('.browse2-result-timestamp-value > '\r\n",
        "                        + '[data-rawdatetime]') \\\r\n",
        "             .get('data-rawdatetime')\r\n",
        "  return {'name': get_text('.browse2-result-name-link'),\r\n",
        "          'link': get_link('.browse2-result-name-link'),\r\n",
        "          'category': get_text('.browse2-result-category'),\r\n",
        "          'type': get_text('.browse2-result-type-name'),\r\n",
        "          'description': get_text('.browse2-result-description'),\r\n",
        "          'tags': [*map(Tag.get_text, element.select('.browse2-result-topic'))],\r\n",
        "          'updated': unix_time_to_iso(int(timestamp)),\r\n",
        "          'apiDocLink': get_link('.browse2-result-api-link')}\r\n",
        "\r\n",
        "# Adds details to each item by modifying its dictionary in-place\r\n",
        "def get_details(data_sets):\r\n",
        "  delay_factor = 1 # This delay factor will increase exponentially on errors.\r\n",
        "  sleep = lambda: time.sleep(randint(2, 4) * delay_factor)\r\n",
        "  for id in data_sets:\r\n",
        "    data_set = data_sets[id]\r\n",
        "    if 'dataDownloads' in data_set or \\\r\n",
        "       'attachments' in data_set or \\\r\n",
        "       'columns' in data_set:\r\n",
        "       continue # Skip items with any of those keys already.\r\n",
        "    try:\r\n",
        "      data_set_name = data_set['name']\r\n",
        "      details_url = data_set['link']\r\n",
        "      print(f'Getting details for {data_set_name} from {details_url}...')\r\n",
        "      sleep() # Take it nice and easy; the server will be angry otherwise.\r\n",
        "      data_set_information, initial_state = parse_details_page(details_url)\r\n",
        "      data_downloads = extract_data_downloads(url, data_set_information)\r\n",
        "      attachments = extract_attachments(url, initial_state)\r\n",
        "      agency = extract_agency(initial_state)\r\n",
        "      columns = extract_column_schema(initial_state)\r\n",
        "      update_frequency = extract_update_frequency(initial_state)\r\n",
        "      data_sets[id]['agency'] = agency\r\n",
        "      data_sets[id]['attachments'] = attachments\r\n",
        "      data_sets[id]['columns'] = columns\r\n",
        "      data_sets[id]['dataDownloads'] = data_downloads\r\n",
        "      data_sets[id]['updateFrequency'] = update_frequency\r\n",
        "      delay_factor = max(1, delay_factor // 2)\r\n",
        "    except ValueError as e:\r\n",
        "      print(f'\\tAn error occured while getting the details: {e}')\r\n",
        "      delay_factor = min(300, delay_factor * 2)\r\n",
        "\r\n",
        "# Extracts the data downloads information associated with the item\r\n",
        "def extract_data_downloads(url_base, data_set_information):\r\n",
        "  if data_set_information and data_set_information['distribution']:\r\n",
        "    return [{'contentUrl': data_download['contentUrl'],\r\n",
        "             'encodingFormat': data_download['encodingFormat']}\r\n",
        "            for data_download\r\n",
        "            in data_set_information['distribution']\r\n",
        "            if data_download['@type'] == 'DataDownload']\r\n",
        "\r\n",
        "# Extracts the attachments information associated with the item\r\n",
        "def extract_attachments(url_base, initial_state):\r\n",
        "  if initial_state and \\\r\n",
        "     initial_state['view'] and \\\r\n",
        "     initial_state['view']['attachments']:\r\n",
        "    return {attachment['name']: urljoin(url_base, attachment['href'])\r\n",
        "            for attachment\r\n",
        "            in initial_state['view']['attachments']}\r\n",
        "\r\n",
        "# Extracts the column schema information associated with the item\r\n",
        "def extract_column_schema(initial_state):\r\n",
        "  if initial_state and \\\r\n",
        "     initial_state['view'] and \\\r\n",
        "     initial_state['view']['columns']:\r\n",
        "    return [{'name': column['fieldName'], # TODO: Refactor this statement; it’s getting complicated.\r\n",
        "             'type': column['dataTypeName'],\r\n",
        "             'humanName': column['name'],\r\n",
        "             'nullCount': get_value(column, 'cachedContents', 'null'),\r\n",
        "             'nonNullCount': get_value(column, 'cachedContents', 'non_null'),\r\n",
        "             'largest': get_value(column, 'cachedContents', 'largest'),\r\n",
        "             'average': get_value(column, 'cachedContents', 'average'),\r\n",
        "             'smallest': get_value(column, 'cachedContents', 'smallest'),\r\n",
        "             'topValues': [*map(lambda _: _['item'],\r\n",
        "                                get_value(column, 'cachedContents', 'top') or [])] or \\\r\n",
        "                          None}\r\n",
        "            for column\r\n",
        "            in sorted(initial_state['view']['columns'],\r\n",
        "                      key=lambda _: int(_['position']))]\r\n",
        "\r\n",
        "# Tries to extract the data set’s agency using one of two ways\r\n",
        "def extract_agency(initial_state):\r\n",
        "  try:\r\n",
        "    result = get_value(initial_state,\r\n",
        "                       'view',\r\n",
        "                       'metadata',\r\n",
        "                       'custom_fields',\r\n",
        "                       'Dataset Information',\r\n",
        "                       'Agency')\r\n",
        "    if result is None:\r\n",
        "      result = get_value(initial_state,\r\n",
        "                         'view',\r\n",
        "                         'coreView',\r\n",
        "                         'metadata',\r\n",
        "                         'custom_fields',\r\n",
        "                         'Dataset Information',\r\n",
        "                         'Agency')\r\n",
        "    return result\r\n",
        "  except TypeError:\r\n",
        "    return None\r\n",
        "\r\n",
        "# Tries to extract the data set’s update frequency using one of two ways\r\n",
        "def extract_update_frequency(initial_state):\r\n",
        "  try:\r\n",
        "    result = get_value(initial_state,\r\n",
        "                       'view',\r\n",
        "                       'metadata',\r\n",
        "                       'custom_fields',\r\n",
        "                       'Update',\r\n",
        "                       'Update Frequency')\r\n",
        "    if result is None:\r\n",
        "      result = get_value(initial_state,\r\n",
        "                         'view',\r\n",
        "                         'coreView',\r\n",
        "                         'metadata',\r\n",
        "                         'custom_fields',\r\n",
        "                         'Update',\r\n",
        "                         'Update Frequency')\r\n",
        "    return result\r\n",
        "  except TypeError:\r\n",
        "    return None\r\n",
        "\r\n",
        "# Gets a value given the keys and indices to it\r\n",
        "def get_value(object, *path_components, default=None):\r\n",
        "    if path_components:\r\n",
        "        try:\r\n",
        "            return get_value(object[path_components[0]],\r\n",
        "                             *path_components[1:],\r\n",
        "                             default=default)\r\n",
        "        except (IndexError, KeyError):\r\n",
        "            return default\r\n",
        "    return object\r\n",
        "\r\n",
        "# Digs into the item’s page to extract additional details\r\n",
        "def parse_details_page(url):\r\n",
        "  response = requests.get(url)\r\n",
        "  response.raise_for_status()\r\n",
        "  soup = BeautifulSoup(response.text)\r\n",
        "  data_set_information = None # The information about the data set embedded in the page\r\n",
        "  json_objects = extract_inline_json(soup)\r\n",
        "  for json_object in json_objects:\r\n",
        "    if json_object and json_object['@type'] == 'Dataset': # Found the information!\r\n",
        "      data_set_information = json_object\r\n",
        "      break\r\n",
        "  initial_state = None # The embedded information that the page uses to initialize its tables\r\n",
        "  scripts = extract_inline_javascript(soup)\r\n",
        "  for script in scripts:\r\n",
        "    try:\r\n",
        "      ast = esprima.parseScript(script, {'range': True}) # Guards against not-JavaScript\r\n",
        "      if ast.type == 'Program' and \\\r\n",
        "         ast.sourceType == 'script' and \\\r\n",
        "         len(ast.body) == 1 and \\\r\n",
        "         ast.body[0].type == 'VariableDeclaration' and \\\r\n",
        "         len(ast.body[0].declarations) == 1: # Found the script with a single variable declaration\r\n",
        "        declaration = ast.body[0].declarations[0]\r\n",
        "        if declaration.type == 'VariableDeclarator' and \\\r\n",
        "           declaration.id.type == 'Identifier' and \\\r\n",
        "           declaration.id.name == 'initialState' and \\\r\n",
        "           declaration.init.type == 'ObjectExpression': # Found the initial state!\r\n",
        "          json_start, json_end = declaration.init.range\r\n",
        "          initial_state = json.loads(script[json_start:json_end])\r\n",
        "          break\r\n",
        "    except Exception as e:\r\n",
        "      pass # Ignore the “script” if parsing it throws an exception.\r\n",
        "  return (data_set_information, initial_state)\r\n",
        "\r\n",
        "# Loads all of the inline JSON found in the page’s script tags\r\n",
        "def extract_inline_json(soup):\r\n",
        "  return [json.loads(element.string)\r\n",
        "          for element\r\n",
        "          in soup.select('script[type=\"application/ld+json\"]')\r\n",
        "          if element.string]\r\n",
        "\r\n",
        "# Loads all of the inline JavaScript found in the page’s script tags\r\n",
        "def extract_inline_javascript(soup):\r\n",
        "  return [element.string\r\n",
        "          for element\r\n",
        "          in soup.select('script')\r\n",
        "          if element.string]\r\n",
        "\r\n",
        "# Gets the URL of the next results page\r\n",
        "def extract_next_url(soup):\r\n",
        "  element = soup.select('a.nextLink')\r\n",
        "  if element:\r\n",
        "    return element[0].get('href')\r\n",
        "\r\n",
        "# Gets the URL of the last results page\r\n",
        "def extract_last_url(soup):\r\n",
        "  element = soup.select('a.lastLink')\r\n",
        "  if element:\r\n",
        "    return element[0].get('href')\r\n",
        "\r\n",
        "# Gets the number of search results to expect\r\n",
        "def extract_expected_length(soup):\r\n",
        "  element = first(soup.select('.browse2-mobile-filter-result-count'))\r\n",
        "  return int(first_nonnull(first(map(re.Match.groups,\r\n",
        "                                     filter(None,\r\n",
        "                                            map(results_count_re.match,\r\n",
        "                                                element.stripped_strings))))))"
      ],
      "outputs": [],
      "metadata": {
        "id": "ub8rvxXfvRsN",
        "cellView": "form"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parsing the Search Results\r\n",
        "The code below extracts the data from the first search results page and all subsequent pages until it cannot find a link to the next page. For each page, it looks for the search result elements and maps each one to a dictionary. The dictionary schema is as follows:\r\n",
        "* **name** (string): the name of the item\r\n",
        "* **link** (string): the link to the page with more information about the item\r\n",
        "* **category** (string): the category of the item (e.g., *Education*)\r\n",
        "* **type** (string): the type of the item (e.g., *Dataset*)\r\n",
        "* **description** (string): a description of the item\r\n",
        "* **tags** (list of strings): a set of tags associated with the item\r\n",
        "* **updated** (string): the timestamp which this item was last updated\r\n",
        "* **apiDocLink** (string): a link to the API documentation (which might possibly be used to extract more metadata about the item)"
      ],
      "metadata": {
        "id": "4FiTW1vjXoPZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "data_sets = load_from_cache(cache) if use_cache == 'Yes' and isfile(cache) else get_data_sets(url)"
      ],
      "outputs": [],
      "metadata": {
        "id": "v8PPw6lnwOO-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22cb392b-f1fe-46ab-d093-0932d816224a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting the Data Set Details\r\n",
        "The code below extracts additional information about the items using the links to the items’ pages. It adds the following keys if the information is available:\r\n",
        "* **columns**: a list of columns and their properties\r\n",
        "    * **name** (string): the name of the column\r\n",
        "    * **humanName** (string): a human-friendly name for the column\r\n",
        "    * **type** (string): the column type (one of: `calendar_date`, `checkbox`, `location`, `number`, `point`, `text`, or `url`)\r\n",
        "    * **nullCount** (number): the count of null values for the column\r\n",
        "    * **nonNullCount** (number): the count of non-null values for the column\r\n",
        "    * **largest**: the largest value for the column\r\n",
        "    * **smallest**: the smallest value for the column\r\n",
        "    * **top** (list): a list of the top values for the column\r\n",
        "* **attachments**: a list of downloadable files—most often spreadsheets or PDFs—that describe the data in more detail\r\n",
        "* **dataDownloads**: a list of downloadable files containing all of the data in different formats\r\n",
        "\r\n",
        "In the case where download of data set details is interrupted, the code will attempt to resume progress. Simply, it checks each dictionary entry for the existence of the above keys. If those don’t exist, it tries to retrieve them and amends the dictionary. Existing keys will not be updated."
      ],
      "metadata": {
        "id": "U8nQnEB4K0Pr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "get_details(data_sets)"
      ],
      "outputs": [],
      "metadata": {
        "id": "2-2QbHRsMS4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Caching Data\r\n",
        "The code below will save a copy of the data to storage for loading and processing by other scripts. The file name is defined in the settings above."
      ],
      "metadata": {
        "id": "bnTGn10FSikm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "with open(cache, 'w') as cache_file:\r\n",
        "  json.dump(data_sets, cache_file, indent=2, sort_keys=True)"
      ],
      "outputs": [],
      "metadata": {
        "id": "UZz_9mslTlNi"
      }
    }
  ]
}