{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "NYC OpenData Data Set Lister.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.8 64-bit ('base': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NYC OpenData: Cluster Identifier\r\n",
        "This script takes a dictionary of data sets scraped from NYC OpenData and tries to identify clusters of similar ones.\r\n",
        "\r\n",
        "Based on the 9/16/2021 version of the scraping tool, the scraped data has the following format:\r\n",
        "* **name** (string): the name of the item\r\n",
        "* **link** (string): the link to the page with more information about the item\r\n",
        "* **category** (string): the category of the item (e.g., *Education*)\r\n",
        "* **type** (string): the type of the item (e.g., *Dataset*)\r\n",
        "* **description** (string): a description of the item\r\n",
        "* **tags** (list of strings): a set of tags associated with the item\r\n",
        "* **updated** (string): the timestamp which this item was last updated\r\n",
        "* **apiDocLink** (string): a link to the API documentation (which might possibly be used to extract more metadata about the item)\r\n",
        "* **columns**: a list of columns and their properties\r\n",
        "    * **name** (string): the name of the column\r\n",
        "    * **humanName** (string): a human-friendly name for the column\r\n",
        "    * **type** (string): the column type (one of: `calendar_date`, `checkbox`, `location`, `number`, `point`, `text`, or `url`)\r\n",
        "* **attachments**: a list of downloadable files—most often spreadsheets or PDFs—that describe the data in more detail\r\n",
        "* **dataDownloads**: a list of downloadable files containing all of the data in different formats\r\n",
        "* **rawMetadata**: a dictionary containing some of the raw data scraped from the page identified by *link*"
      ],
      "metadata": {
        "id": "YckoFp3Zvm7E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing and Loading Libraries"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Natural Language Toolkit (NLTK)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install nltk\r\n",
        "\r\n",
        "import nltk\r\n",
        "from nltk.corpus import wordnet\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "\r\n",
        "nltk.download('averaged_perceptron_tagger') # Needed for part of speech tagger\r\n",
        "nltk.download('punkt') # Needed for lemmatizer\r\n",
        "nltk.download('wordnet')\r\n",
        "\r\n",
        "lemmatize_word = WordNetLemmatizer().lemmatize"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### sci-kit learn"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.cluster import AgglomerativeClustering"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Usual"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import re\r\n",
        "import json\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from functools import partial, reduce\r\n",
        "from itertools import chain, combinations, permutations, starmap"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extra Functions"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def apply_1(func, arg): # Applies 1 argument to a function\r\n",
        "    return func(arg)\r\n",
        "\r\n",
        "def compose(*funcs): # Composes one or more functions into one new function\r\n",
        "    return reduce(lambda f, g: lambda x: f(g(x)), funcs)\r\n",
        "\r\n",
        "def flat_map(func, iterable): # Flattens a sequence of sequences down one dimention and maps each of the elements\r\n",
        "    return map(func, chain.from_iterable(iterable))\r\n",
        "\r\n",
        "def map_apply(funcs, iterable): # Applies a sequence of functions to a sequence of sequences\r\n",
        "    for args in iterable:\r\n",
        "        yield tuple(starmap(apply_1, zip(funcs, args)))\r\n",
        "\r\n",
        "def map_collect(map_func, collect_func, iterable): # Maps a sequence and applies the collect function to the result\r\n",
        "    return collect_func(map(map_func, iterable))\r\n",
        "\r\n",
        "def pairs(iterable): # Creates every 2-element permutation of a sequence\r\n",
        "    return permutations(iterable, 2)\r\n",
        "\r\n",
        "def first(iterable, default=None): # Gets the first element of a sequence or a default value\r\n",
        "    return next(iter(iterable), default)\r\n",
        "\r\n",
        "def second(iterable, default=None): # Gets the second element of a sequence or a default value\r\n",
        "    iterator = iter(iterable)\r\n",
        "    next(iterator, None)\r\n",
        "    return next(iterator, default)\r\n",
        "\r\n",
        "def zip_to_dict(keys, values): # Creates a new dictionary from a sequence of keys and a sequence of values\r\n",
        "    return dict(zip(keys, values))\r\n",
        "\r\n",
        "def load_from_cache(path): # Loads a JSON file from a given path\r\n",
        "    with open(path, 'r') as cache_file:\r\n",
        "        return json.load(cache_file)"
      ],
      "outputs": [],
      "metadata": {
        "id": "ub8rvxXfvRsN",
        "cellView": "form"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Settings\r\n",
        "cache_path = \"cache.json\" #@param {type:\"string\"}\r\n",
        "\r\n",
        "# Load the data sets.\r\n",
        "data_sets = load_from_cache(cache_path)\r\n",
        "\r\n",
        "# Display the names.\r\n",
        "for data_set in data_sets.values():\r\n",
        "    print('*', data_set['name'])"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization"
      ],
      "metadata": {
        "id": "4FiTW1vjXoPZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Helper functions for lemmatization\r\n",
        "select_words = re.compile(r'yyyy(?:-yyyy)?|[0-9A-Za-z]+(?:\\'s)?') \\\r\n",
        "                 .findall # re produces better results than nltk.tokenize.\r\n",
        "\r\n",
        "substitute_years = compose( # Reduces years to a binary distinction, so different years have an edit distance of 0\r\n",
        "    partial(re.compile(r'(?:^|\\D)\\d\\d(?:\\d\\d)?(?:$|\\D)').sub, ' yyyy '), # Just a year\r\n",
        "    partial(re.compile(r'(?:^|\\D)\\d\\d(?:\\d\\d)\\s*-\\s*\\d\\d(?:\\d\\d)?(?:$|\\D)').sub, ' yyyy-yyyy ')) # A year range\r\n",
        "\r\n",
        "def selectively_lower(value): # Lowercases strings that don’t look like acronyms\r\n",
        "    if len(value) <= 3 or \\\r\n",
        "       value in ('yyyy', 'yyyy-yyyy') or \\\r\n",
        "       re.match(r'^.$|[A-Z][^a-z]|.[A-Z]$', value):\r\n",
        "        return value\r\n",
        "    return value.lower()\r\n",
        "\r\n",
        "def convert_pos_tag_to_wordnet(tag): # Converts NLTK POS tags to WordNet POS tags\r\n",
        "    tag = tag.upper()\r\n",
        "    if tag in (',', ':', '(', ')', 'CC', 'CD', 'DT', 'IN', 'POS', 'TO'):\r\n",
        "        return None # No POS—to ignore words that don’t contribute meaningfully to name comparison\r\n",
        "    elif tag[0] == 'J':\r\n",
        "        return wordnet.ADJ\r\n",
        "    elif tag[0] == 'N':\r\n",
        "        return wordnet.NOUN\r\n",
        "    elif tag[0] == 'V':\r\n",
        "        return wordnet.VERB\r\n",
        "    elif tag[0] == 'R':\r\n",
        "        return wordnet.ADV\r\n",
        "    return wordnet.NOUN\r\n",
        "\r\n",
        "def lemmatize_expression(expression): # Lemmatizes a string of words\r\n",
        "    tokens = [*filter(None, # Filters out empty strings\r\n",
        "                      select_words(substitute_years(expression)))]\r\n",
        "    tagged_tokens = map_apply((selectively_lower,\r\n",
        "                               convert_pos_tag_to_wordnet),\r\n",
        "                              nltk.pos_tag(tokens))\r\n",
        "    return [*starmap(lemmatize_word, # Feeds the converted token-POS pairs through the WordNet lemmatizer\r\n",
        "                     filter(second, tagged_tokens))] # Filters out token-POS pairs with no POS\r\n",
        "\r\n",
        "# Create a dictionary of data sets with their lemmatized names.\r\n",
        "lemmatized_names = {identifier: lemmatize_expression(data_set['name'])\r\n",
        "                    for identifier, data_set\r\n",
        "                    in data_sets.items()}\r\n",
        "\r\n",
        "# Display the unique lemmas found in the data set names.\r\n",
        "unique_lemmas = {*flat_map(str.casefold,\r\n",
        "                           lemmatized_names.values())}\r\n",
        "print(f'{len(unique_lemmas):,} lemmas identified in {len(lemmatized_names):,} names:')\r\n",
        "print(', '.join(sorted(unique_lemmas)))"
      ],
      "outputs": [],
      "metadata": {
        "id": "v8PPw6lnwOO-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22cb392b-f1fe-46ab-d093-0932d816224a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Edit Distances"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Constants\r\n",
        "INT_INF = 2147483647 # A stand-in for “very large number” that’s not a float; needed for the clustering algorithm\r\n",
        "\r\n",
        "# First pass: compute the edit distance of all name pairs.\r\n",
        "edit_distance_dict = zip_to_dict(pairs(lemmatized_names.keys()),\r\n",
        "                                 starmap(nltk.edit_distance,\r\n",
        "                                         pairs(lemmatized_names.values())))\r\n",
        "\r\n",
        "# Second pass: set the edit distance of names with disjoint sets of lemmas to “infinity.”\r\n",
        "# Names with few words will have short edit distances even though they have nothing in common, and\r\n",
        "# resetting their distances to infinity lowers the ranking of such pairs, pushing them to the bottom.\r\n",
        "for key1, key2 in edit_distance_dict:\r\n",
        "    if set(lemmatized_names[key1]).isdisjoint(lemmatized_names[key2]):\r\n",
        "        edit_distance_dict[(key1, key2)] = INT_INF\r\n",
        "\r\n",
        "# Display the stats.\r\n",
        "print(f'{len(edit_distance_dict):,} pairs of names measured')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Get the pairs with the minimum edit distance for any data set name.\r\n",
        "def to_min_edit_distance_dict(edit_distance_dict):\r\n",
        "    minimums = {} # Keeps track of the minimum edit distance for each name\r\n",
        "    result = {}\r\n",
        "    for (key1, key2), value in sorted(edit_distance_dict.items(), key=second):\r\n",
        "        if value <= minimums.get(key1, INT_INF) and value <= minimums.get(key2, INT_INF):\r\n",
        "            result[(key1, key2)] = value\r\n",
        "            minimums[key1] = value\r\n",
        "            minimums[key2] = value\r\n",
        "    return result\r\n",
        "\r\n",
        "min_edit_distance_dict = to_min_edit_distance_dict(edit_distance_dict)\r\n",
        "\r\n",
        "# Display the edit distance stats.\r\n",
        "# Note that each data set may have the same edit distance to more than one other data set,\r\n",
        "# so the grand total of counts will be substantially more than the number of data sets.\r\n",
        "edit_distance_counts = pd.Series(min_edit_distance_dict.values(),\r\n",
        "                                 index=min_edit_distance_dict.keys()) \\\r\n",
        "                         .value_counts() \\\r\n",
        "                         .to_frame('Count') \\\r\n",
        "                         .sort_index() \\\r\n",
        "                         .sort_values(by='Count',\r\n",
        "                                      ascending=False)\r\n",
        "edit_distance_counts.index.set_names('Edit Distance', inplace=True)\r\n",
        "edit_distance_counts.style.format('{:,}'.format)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Create a distance matrix from the edit distance dictionary.\r\n",
        "D = pd.DataFrame(edit_distance_dict.values(),\r\n",
        "                 index=edit_distance_dict.keys(),\r\n",
        "                 columns=['Edit Distance']) \\\r\n",
        "      .unstack(1) \\\r\n",
        "      .droplevel(0, 1) \\\r\n",
        "      .fillna(0) # A name’s edit distance to itself will be 0.\r\n",
        "\r\n",
        "# Cluster the data sets using the edit distances.\r\n",
        "agg = AgglomerativeClustering(n_clusters=None,\r\n",
        "                              distance_threshold=2, # Casts a wide net, but the clusters will be further refined.\r\n",
        "                              affinity='precomputed',\r\n",
        "                              linkage ='average')\r\n",
        "clusters = pd.DataFrame(agg.fit_predict(D),\r\n",
        "                        index=D.index,\r\n",
        "                        columns=['cluster']) \\\r\n",
        "             .reset_index() \\\r\n",
        "             .groupby('cluster') \\\r\n",
        "             ['index'] \\\r\n",
        "             .agg(set) \\\r\n",
        "             .to_list()\r\n",
        "\r\n",
        "# Sort the clusters by number of data sets within each of them.\r\n",
        "clusters.sort(key=len, reverse=True)\r\n",
        "\r\n",
        "# Display the cluster stats.\r\n",
        "print(f'{len(clusters)} clusters in total')\r\n",
        "print()\r\n",
        "print('Top 3 clusters by data set count:')\r\n",
        "for index in range(min(len(clusters), 3)):\r\n",
        "    print(f'Cluster #{index}: {len(clusters[index]):,} data sets')\r\n",
        "    print('*',\r\n",
        "          map_collect(lambda identifier: data_sets[identifier]['name'],\r\n",
        "                      '\\n* '.join,\r\n",
        "                      clusters[index]))"
      ],
      "outputs": [],
      "metadata": {}
    }
  ]
}