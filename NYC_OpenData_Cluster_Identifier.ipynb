{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "NYC OpenData Data Set Lister.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.8 64-bit ('base': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NYC OpenData: Cluster Identifier\r\n",
        "This script takes a dictionary of data sets scraped from NYC OpenData and tries to identify clusters of similar ones.\r\n",
        "\r\n",
        "Based on the 9/27/2021 version of the scraping tool, the scraped data has the following format:\r\n",
        "* **name** (string): the name of the item\r\n",
        "* **link** (string): the link to the page with more information about the item\r\n",
        "* **category** (string): the category of the item (e.g., *Education*)\r\n",
        "* **type** (string): the type of the item (e.g., *Dataset*)\r\n",
        "* **description** (string): a description of the item\r\n",
        "* **tags** (list of strings): a set of tags associated with the item\r\n",
        "* **updated** (string): the timestamp which this item was last updated\r\n",
        "* **updateFrequency** (string): the frequency the data set is updated if not *Historical Data*\r\n",
        "* **apiDocLink** (string): a link to the API documentation\r\n",
        "* **columns**: a list of columns and their properties\r\n",
        "    * **name** (string): the name of the column\r\n",
        "    * **humanName** (string): a human-friendly name for the column\r\n",
        "    * **type** (string): the column type (one of: `calendar_date`, `checkbox`, `location`, `number`, `point`, `text`, or `url`)\r\n",
        "    * **nullCount** (number): the count of null values for the column\r\n",
        "    * **nonNullCount** (number): the count of non-null values for the column\r\n",
        "    * **largest**: the largest value for the column\r\n",
        "    * **smallest**: the smallest value for the column\r\n",
        "    * **top** (list): a list of the top values for the column\r\n",
        "* **attachments**: a list of downloadable files—most often spreadsheets or PDFs—that describe the data in more detail\r\n",
        "* **dataDownloads**: a list of downloadable files containing all of the data in different formats"
      ],
      "metadata": {
        "id": "YckoFp3Zvm7E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing and Loading Libraries"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Natural Language Toolkit (NLTK)\r\n",
        "NLTK provides the part of speech (POS) tagger and lemmatizer which are needed to simplify the words in the data set names to their most basic forms. The simplification facilitates the clustering process when the edit distance algorithm is applied to the data set names."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install nltk\r\n",
        "\r\n",
        "import nltk\r\n",
        "from nltk.corpus import wordnet\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "\r\n",
        "nltk.download('averaged_perceptron_tagger') # Needed for part of speech tagger\r\n",
        "nltk.download('punkt') # Needed for lemmatizer\r\n",
        "nltk.download('wordnet') # Also needed for the lemmatizer\r\n",
        "\r\n",
        "lemmatize_word = WordNetLemmatizer().lemmatize"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Other Libraries"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import re\r\n",
        "import json\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from functools import partial, reduce\r\n",
        "from itertools import chain, combinations, permutations, starmap\r\n",
        "from sklearn.cluster import AgglomerativeClustering"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extra Functions\r\n",
        "Most of these are extra functions for facilitating a functional programming style and/or making the code more readable by encapsulating complicated patterns.\r\n",
        "\r\n",
        "`load_from_cache` is the function that loads the data to be processed from the file system."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def apply_1(func, arg): # Applies 1 argument to a function\r\n",
        "    return func(arg)\r\n",
        "\r\n",
        "def compose(*funcs): # Composes one or more functions into one new function\r\n",
        "    return reduce(lambda f, g: lambda x: f(g(x)), funcs)\r\n",
        "\r\n",
        "def flat_map(func, iterable): # Flattens a sequence of sequences down one dimention and maps each of the elements\r\n",
        "    return map(func, chain.from_iterable(iterable))\r\n",
        "\r\n",
        "def map_apply(funcs, iterable): # Applies a sequence of functions to a sequence of sequences\r\n",
        "    for args in iterable:\r\n",
        "        yield tuple(starmap(apply_1, zip(funcs, args)))\r\n",
        "\r\n",
        "def map_collect(map_func, collect_func, iterable): # Maps a sequence and applies the collect function to the result\r\n",
        "    return collect_func(map(map_func, iterable))\r\n",
        "\r\n",
        "def pairs(iterable): # Creates every 2-element permutation of a sequence\r\n",
        "    return permutations(iterable, 2)\r\n",
        "\r\n",
        "def first(iterable, default=None): # Gets the first element of a sequence or a default value\r\n",
        "    return next(iter(iterable), default)\r\n",
        "\r\n",
        "def second(iterable, default=None): # Gets the second element of a sequence or a default value\r\n",
        "    iterator = iter(iterable)\r\n",
        "    next(iterator, None)\r\n",
        "    return next(iterator, default)\r\n",
        "\r\n",
        "def zip_to_dict(keys, values): # Creates a new dictionary from a sequence of keys and a sequence of values\r\n",
        "    return dict(zip(keys, values))\r\n",
        "\r\n",
        "def load_from_cache(path): # Loads a JSON file from a given path\r\n",
        "    with open(path, 'r') as cache_file:\r\n",
        "        return json.load(cache_file)"
      ],
      "outputs": [],
      "metadata": {
        "id": "ub8rvxXfvRsN",
        "cellView": "form"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Settings\r\n",
        "cache_path = \"cache.json\" #@param {type:\"string\"}\r\n",
        "\r\n",
        "# Load the data sets.\r\n",
        "data_sets = load_from_cache(cache_path)\r\n",
        "\r\n",
        "# Display a preview of the data—no fancy processing.\r\n",
        "pd.DataFrame.from_dict(data_sets,\r\n",
        "                       orient='index',\r\n",
        "                       columns=['name',\r\n",
        "                                'type',\r\n",
        "                                'category',\r\n",
        "                                'tags',\r\n",
        "                                'updated',\r\n",
        "                                'updateFrequency',\r\n",
        "                                'description',\r\n",
        "                                'link',\r\n",
        "                                'apiDocLink',\r\n",
        "                                'columns',\r\n",
        "                                'attachments',\r\n",
        "                                'dataDownloads'])"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization\r\n",
        "Lemmatization reduces different forms of the same words in the data set names to their root forms. For example, “school” and “schools” both reduce to “school.” This makes the names more comparable by the edit distance algorithm."
      ],
      "metadata": {
        "id": "4FiTW1vjXoPZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Helper functions for lemmatization\r\n",
        "select_words = re.compile(r'yyyy(?:-yyyy)?|[0-9A-Za-z]+(?:\\'s)?') \\\r\n",
        "                 .findall # re produces better results than nltk.tokenize.\r\n",
        "\r\n",
        "substitute_years = compose( # Reduces years to a binary distinction, so different years have an edit distance of 0\r\n",
        "    partial(re.compile(r'(?:^|\\D)\\d\\d(?:\\d\\d)?(?:$|\\D)').sub, ' yyyy '), # Just a year\r\n",
        "    partial(re.compile(r'(?:^|\\D)\\d\\d(?:\\d\\d)\\s*-\\s*\\d\\d(?:\\d\\d)?(?:$|\\D)').sub, ' yyyy-yyyy ')) # A year range\r\n",
        "\r\n",
        "def selectively_lower(value): # Lowercases strings that don’t look like acronyms\r\n",
        "    if len(value) <= 3 or \\\r\n",
        "       value in ('yyyy', 'yyyy-yyyy') or \\\r\n",
        "       re.match(r'^.$|[A-Z][^a-z]|.[A-Z]$', value):\r\n",
        "        return value\r\n",
        "    return value.lower()\r\n",
        "\r\n",
        "def convert_pos_tag_to_wordnet(tag): # Converts NLTK POS tags to WordNet POS tags\r\n",
        "    tag = tag.upper()\r\n",
        "    if tag in (',', ':', '(', ')', 'CC', 'CD', 'DT', 'IN', 'POS', 'TO'):\r\n",
        "        return None # No POS—to ignore words that don’t contribute meaningfully to name comparison\r\n",
        "    elif tag[0] == 'J':\r\n",
        "        return wordnet.ADJ\r\n",
        "    elif tag[0] == 'N':\r\n",
        "        return wordnet.NOUN\r\n",
        "    elif tag[0] == 'V':\r\n",
        "        return wordnet.VERB\r\n",
        "    elif tag[0] == 'R':\r\n",
        "        return wordnet.ADV\r\n",
        "    return wordnet.NOUN\r\n",
        "\r\n",
        "def lemmatize_expression(expression): # Lemmatizes a string of words\r\n",
        "    tokens = [*filter(None, # Filters out empty strings\r\n",
        "                      select_words(substitute_years(expression)))]\r\n",
        "    tagged_tokens = map_apply((selectively_lower,\r\n",
        "                               convert_pos_tag_to_wordnet),\r\n",
        "                              nltk.pos_tag(tokens))\r\n",
        "    return [*starmap(lemmatize_word, # Feeds the converted token-POS pairs through the WordNet lemmatizer\r\n",
        "                     filter(second, tagged_tokens))] # Filters out token-POS pairs with no POS\r\n",
        "\r\n",
        "# Create a dictionary of data sets with their lemmatized names.\r\n",
        "lemmatized_names = {identifier: lemmatize_expression(data_set['name'])\r\n",
        "                    for identifier, data_set\r\n",
        "                    in data_sets.items()}\r\n",
        "\r\n",
        "# Display the unique lemmas found in the data set names.\r\n",
        "unique_lemmas = {*flat_map(str.casefold,\r\n",
        "                           lemmatized_names.values())}\r\n",
        "print(f'{len(unique_lemmas):,} lemmas identified in {len(lemmatized_names):,} names:')\r\n",
        "print(', '.join(sorted(unique_lemmas)))"
      ],
      "outputs": [],
      "metadata": {
        "id": "v8PPw6lnwOO-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22cb392b-f1fe-46ab-d093-0932d816224a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Edit Distances\r\n",
        "The edit distance is a measure of how many insertion and deletion operations are needed to transform one string into another. Strings that are similar with have shorter edit distances and strings that are dissimilar have longer edit distances.\r\n",
        "\r\n",
        "The edit distance dictionary built here will operate on strings of *words* and not strings of *characters*. Comparing names that contain the words “read” and “reads” (edit distance of 0) is more meaningful than comparing names that contain the character strings “read” and “red” (edit distance 1). In the former case, the words are related, but in the latter case, the words are unrelated."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Constants\r\n",
        "INT_INF = 2147483647 # A stand-in for “very large number” that’s not a float; needed for the clustering algorithm\r\n",
        "\r\n",
        "# First pass: compute the edit distance of all name pairs.\r\n",
        "edit_distance_dict = zip_to_dict(pairs(lemmatized_names.keys()),\r\n",
        "                                 starmap(nltk.edit_distance,\r\n",
        "                                         pairs(lemmatized_names.values())))\r\n",
        "\r\n",
        "# Second pass: set the edit distance of names with disjoint sets of lemmas to “infinity.”\r\n",
        "# Names with few words will have short edit distances even though they have nothing in common, and\r\n",
        "# resetting their distances to infinity lowers the ranking of such pairs, pushing them to the bottom.\r\n",
        "for key1, key2 in edit_distance_dict:\r\n",
        "    if set(lemmatized_names[key1]).isdisjoint(lemmatized_names[key2]):\r\n",
        "        edit_distance_dict[(key1, key2)] = INT_INF\r\n",
        "\r\n",
        "# Display the stats.\r\n",
        "print(f'{len(edit_distance_dict):,} pairs of names measured')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering\r\n",
        "Using the edit distance and agglomerative clusterer, groups of data sets with similar names can be automatically created."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Minimum Edit Distances\r\n",
        "The cell below shows the minimum edit distances found so far."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Get the pairs with the minimum edit distance for any data set name.\r\n",
        "def to_min_edit_distance_dict(edit_distance_dict):\r\n",
        "    minimums = {} # Keeps track of the minimum edit distance for each name\r\n",
        "    result = {}\r\n",
        "    for (key1, key2), value in sorted(edit_distance_dict.items(), key=second):\r\n",
        "        if value <= minimums.get(key1, INT_INF) and value <= minimums.get(key2, INT_INF):\r\n",
        "            result[(key1, key2)] = value\r\n",
        "            minimums[key1] = value\r\n",
        "            minimums[key2] = value\r\n",
        "    return result\r\n",
        "\r\n",
        "min_edit_distance_dict = to_min_edit_distance_dict(edit_distance_dict)\r\n",
        "\r\n",
        "# Display the edit distance stats.\r\n",
        "# Note that each data set may have the same edit distance to more than one other data set,\r\n",
        "# so the grand total of counts will be substantially more than the number of data sets.\r\n",
        "edit_distance_counts = pd.Series(min_edit_distance_dict.values(),\r\n",
        "                                 index=min_edit_distance_dict.keys()) \\\r\n",
        "                         .value_counts() \\\r\n",
        "                         .to_frame('Count') \\\r\n",
        "                         .sort_index() \\\r\n",
        "                         .sort_values(by='Count',\r\n",
        "                                      ascending=False)\r\n",
        "edit_distance_counts.index.set_names('Edit Distance', inplace=True)\r\n",
        "edit_distance_counts.style.format('{:,}'.format)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Found Clusters\r\n",
        "The cell below finds the clusters."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Create a distance matrix from the edit distance dictionary.\r\n",
        "D = pd.DataFrame(edit_distance_dict.values(),\r\n",
        "                 index=edit_distance_dict.keys(),\r\n",
        "                 columns=['Edit Distance']) \\\r\n",
        "      .unstack(1) \\\r\n",
        "      .droplevel(0, 1) \\\r\n",
        "      .fillna(0) # A name’s edit distance to itself will be 0.\r\n",
        "\r\n",
        "# Cluster the data sets using the edit distances.\r\n",
        "agg = AgglomerativeClustering(n_clusters=None,\r\n",
        "                              distance_threshold=2, # Casts a wide net, but the clusters will be further refined.\r\n",
        "                              affinity='precomputed',\r\n",
        "                              linkage ='average')\r\n",
        "clusters = pd.DataFrame(agg.fit_predict(D),\r\n",
        "                        index=pd.Index(D.index, name='identifier'),\r\n",
        "                        columns=['cluster']) \\\r\n",
        "             .sort_values(by='cluster') \\\r\n",
        "             .reset_index()\r\n",
        "\r\n",
        "# Display the cluster count.\r\n",
        "print('Count of each cluster:')\r\n",
        "clusters.cluster \\\r\n",
        "        .value_counts() \\\r\n",
        "        .to_frame('count')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Postprocesing\r\n",
        "Now the cluster data can be further processed to do other things."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pivot\r\n",
        "Generates a table of clusters with data set identifiers belonging to the cluster going across each row."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pd.DataFrame(clusters.groupby('cluster') \\\r\n",
        "                     .agg({'identifier': list}) \\\r\n",
        "                     .identifier.to_list(),\r\n",
        "             index=pd.RangeIndex(start=0,\r\n",
        "                                 stop=clusters.cluster.nunique(),\r\n",
        "                                 name='cluster'))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating an Organized Directory Structure with JSON Files\r\n",
        "Instead of one big JSON file, organize the data into individual JSON files—one per data set—and directories—one per cluster. Clusters of 1, however, will be lumped together into a “cluster-misc” directory since it’d be wasteful to create 1 directory per file."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from os import makedirs\r\n",
        "from os.path import join\r\n",
        "\r\n",
        "for cluster, identifiers in enumerate(clusters.groupby('cluster').identifier.agg(list).to_list()):\r\n",
        "    directory_name = f'cluster-{cluster}' \\\r\n",
        "                     if len(identifiers) > 1 \\\r\n",
        "                     else 'cluster-misc'\r\n",
        "    makedirs(directory_name, exist_ok=True)\r\n",
        "    print(f'Created cluster directory “{directory_name}”.')\r\n",
        "    for identifier in identifiers:\r\n",
        "        file_path = join(directory_name, f'{identifier}.json')\r\n",
        "        data_set = data_sets[identifier]\r\n",
        "        with open(file_path, 'w') as file:\r\n",
        "            json.dump(data_set, file, indent=4)\r\n",
        "        print(f'* Wrote metadata of “{data_set[\"name\"]}” into “{file_path}”.')"
      ],
      "outputs": [],
      "metadata": {}
    }
  ]
}